#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ‰å£°ä¹¦å¹¿å‘ŠåŒ¹é…ç³»ç»Ÿè¯„ä¼°è„šæœ¬
è¿è¡Œå®Œæ•´çš„ç³»ç»Ÿè¯„ä¼°ï¼Œç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from main import AudioBookAdPipeline
from src.monitoring_analysis.evaluation_metrics import EvaluationMetrics
from src.monitoring_analysis.performance_analyzer import PerformanceAnalyzer

def run_comprehensive_evaluation():
    """è¿è¡Œå…¨é¢çš„ç³»ç»Ÿè¯„ä¼°"""
    print("ğŸ” æœ‰å£°ä¹¦å¹¿å‘ŠåŒ¹é…ç³»ç»Ÿ - å…¨é¢è¯„ä¼°")
    print("=" * 60)
    
    try:
        # åˆ›å»ºPipelineå®ä¾‹
        print("ğŸ“‹ åˆå§‹åŒ–ç³»ç»Ÿ...")
        pipeline = AudioBookAdPipeline()
        
        # å¯åŠ¨ç›‘æ§
        print("ğŸ“Š å¯åŠ¨æ€§èƒ½ç›‘æ§...")
        pipeline.start_monitoring()
        
        # è¿è¡Œè¯„ä¼°
        print("\nğŸš€ å¼€å§‹ç³»ç»Ÿè¯„ä¼°...")
        evaluation_report = pipeline.run_evaluation()
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š è¯„ä¼°ç­‰çº§: {evaluation_report['summary']['overall_grade']}")
        print(f"â­ æ€»ä½“è¯„åˆ†: {evaluation_report['summary']['overall_score']:.2f}/4.0")
        
        # æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡
        print(f"\nğŸ“ˆ æŒ‡æ ‡ç»Ÿè®¡:")
        print(f"  ä¼˜ç§€æŒ‡æ ‡: {evaluation_report['summary']['excellent_count']}ä¸ª")
        print(f"  è‰¯å¥½æŒ‡æ ‡: {evaluation_report['summary']['good_count']}ä¸ª")
        print(f"  å¯æ¥å—æŒ‡æ ‡: {evaluation_report['summary']['acceptable_count']}ä¸ª")
        print(f"  éœ€æ”¹è¿›æŒ‡æ ‡: {evaluation_report['summary']['needs_improvement_count']}ä¸ª")
        
        # æ˜¾ç¤ºè¯¦ç»†æŒ‡æ ‡åˆ†æ
        print(f"\nğŸ” è¯¦ç»†æŒ‡æ ‡åˆ†æ:")
        for metric_name, analysis in evaluation_report['benchmark_analysis'].items():
            status_emoji = {
                "excellent": "ğŸŸ¢",
                "good": "ğŸŸ¡", 
                "acceptable": "ğŸŸ ",
                "needs_improvement": "ğŸ”´"
            }
            emoji = status_emoji.get(analysis['status'], "âšª")
            
            print(f"  {emoji} {metric_name}:")
            print(f"    å½“å‰å€¼: {analysis['current_value']:.3f}")
            print(f"    è¡Œä¸šæ ‡å‡†: {analysis['benchmark_value']:.3f}")
            print(f"    æ”¹è¿›ç©ºé—´: {analysis['improvement_percent']:+.1f}%")
            print(f"    çŠ¶æ€: {analysis['status']}")
        
        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        if evaluation_report['recommendations']:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(evaluation_report['recommendations']):
                print(f"  {i+1}. {rec}")
        else:
            print(f"\nğŸ‰ æ­å–œ! æ‰€æœ‰æŒ‡æ ‡éƒ½è¾¾åˆ°äº†è‰¯å¥½æˆ–ä¼˜ç§€æ°´å¹³!")
        
        # ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ
        print(f"\nğŸš€ ä¼˜åŒ–æ–¹æ¡ˆ:")
        optimization_plan = generate_optimization_plan(evaluation_report)
        for i, plan in enumerate(optimization_plan):
            print(f"  {i+1}. {plan}")
        
        # ä¿å­˜ä¼˜åŒ–æ–¹æ¡ˆ
        save_optimization_plan(optimization_plan, evaluation_report)
        
        # è¿è¡Œæ·±åº¦æ€§èƒ½åˆ†æ
        print(f"\nğŸ” æ·±åº¦æ€§èƒ½åˆ†æ:")
        run_deep_performance_analysis(evaluation_report)
        
        # åœæ­¢ç›‘æ§
        pipeline.stop_monitoring()
        
        print(f"\nğŸ“ è¯„ä¼°æŠ¥å‘Šå’Œä¼˜åŒ–æ–¹æ¡ˆå·²ä¿å­˜åˆ°logsç›®å½•")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def generate_optimization_plan(evaluation_report: dict) -> list:
    """æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ"""
    plans = []
    
    # åˆ†æç‚¹å‡»ç‡
    ctr_analysis = evaluation_report['benchmark_analysis'].get('click_through_rate', {})
    if ctr_analysis.get('status') in ['needs_improvement', 'acceptable']:
        plans.append("ä¼˜åŒ–æ¨èç®—æ³•ï¼Œæé«˜å†…å®¹ä¸å¹¿å‘Šçš„åŒ¹é…ç²¾åº¦")
        plans.append("æ”¹è¿›ç”¨æˆ·ç”»åƒå»ºæ¨¡ï¼Œå¢å¼ºä¸ªæ€§åŒ–æ¨èèƒ½åŠ›")
        plans.append("ä¼˜åŒ–å¹¿å‘Šåˆ›æ„å±•ç¤ºï¼Œæå‡ç”¨æˆ·ç‚¹å‡»æ„æ„¿")
    
    # åˆ†æè½¬åŒ–ç‡
    conversion_analysis = evaluation_report['benchmark_analysis'].get('conversion_rate', {})
    if conversion_analysis.get('status') in ['needs_improvement', 'acceptable']:
        plans.append("ä¼˜åŒ–æŠ•æ”¾æ—¶æœºï¼Œé€‰æ‹©ç”¨æˆ·æœ€æ´»è·ƒçš„æ—¶é—´æ®µ")
        plans.append("æ”¹è¿›å¹¿å‘Šæ–‡æ¡ˆï¼Œå¢å¼ºäº§å“å¸å¼•åŠ›")
        plans.append("ä¼˜åŒ–è½åœ°é¡µä½“éªŒï¼Œå‡å°‘è½¬åŒ–æ¼æ–—æµå¤±")
    
    # åˆ†ææ”¶å…¥æŒ‡æ ‡
    revenue_analysis = evaluation_report['benchmark_analysis'].get('revenue_per_user', {})
    if revenue_analysis.get('status') in ['needs_improvement', 'acceptable']:
        plans.append("ä¼˜åŒ–å®šä»·ç­–ç•¥ï¼Œæé«˜å•ç”¨æˆ·ä»·å€¼")
        plans.append("æ‰©å±•äº§å“çº¿ï¼Œå¢åŠ äº¤å‰é”€å”®æœºä¼š")
        plans.append("æ”¹è¿›å®¢æˆ·æœåŠ¡ï¼Œæå‡ç”¨æˆ·å¿ è¯šåº¦")
    
    # åˆ†æå“åº”æ—¶é—´
    latency_analysis = evaluation_report['benchmark_analysis'].get('response_time', {})
    if latency_analysis.get('status') in ['needs_improvement', 'acceptable']:
        plans.append("ä¼˜åŒ–ç®—æ³•æ€§èƒ½ï¼Œå‡å°‘è®¡ç®—å»¶è¿Ÿ")
        plans.append("æ”¹è¿›ç¼“å­˜ç­–ç•¥ï¼Œæå‡å“åº”é€Ÿåº¦")
        plans.append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œå‡å°‘I/Oç­‰å¾…")
    
    # åˆ†æç”¨æˆ·æ»¡æ„åº¦
    satisfaction_analysis = evaluation_report['benchmark_analysis'].get('user_satisfaction', {})
    if satisfaction_analysis.get('status') in ['needs_improvement', 'acceptable']:
        plans.append("æ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒæŒç»­æ”¹è¿›äº§å“ä½“éªŒ")
        plans.append("ä¼˜åŒ–ç”¨æˆ·ç•Œé¢ï¼Œæå‡æ“ä½œä¾¿åˆ©æ€§")
        plans.append("åŠ å¼ºå†…å®¹è´¨é‡ï¼Œæ»¡è¶³ç”¨æˆ·éœ€æ±‚")
    
    # å¦‚æœæ²¡æœ‰å…·ä½“é—®é¢˜ï¼Œæä¾›é€šç”¨ä¼˜åŒ–å»ºè®®
    if not plans:
        plans = [
            "æŒç»­ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼ŒåŠæ—¶å‘ç°æ½œåœ¨é—®é¢˜",
            "å®šæœŸæ›´æ–°æ¨èæ¨¡å‹ï¼Œé€‚åº”å¸‚åœºå˜åŒ–",
            "ä¼˜åŒ–æŠ•æ”¾ç­–ç•¥ï¼Œæå‡å¹¿å‘Šæ•ˆæœ",
            "åŠ å¼ºæ•°æ®åˆ†æï¼ŒæŒ–æ˜ç”¨æˆ·è¡Œä¸ºæ´å¯Ÿ"
        ]
    
    return plans

def run_deep_performance_analysis(evaluation_report: dict):
    """è¿è¡Œæ·±åº¦æ€§èƒ½åˆ†æ"""
    try:
        # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
        analyzer = PerformanceAnalyzer()
        
        # æå–æŒ‡æ ‡æ•°æ®
        metrics_data = {}
        for metric_name, analysis in evaluation_report['benchmark_analysis'].items():
            metrics_data[metric_name] = analysis['current_value']
        
        # åˆ†ææ€§èƒ½é—®é¢˜
        issues = analyzer.analyze_performance_issues(metrics_data)
        
        if issues:
            print(f"  ğŸ“Š å‘ç° {len(issues)} ä¸ªæ€§èƒ½é—®é¢˜:")
            for i, issue in enumerate(issues):
                severity_emoji = {
                    "critical": "ğŸ”´",
                    "high": "ğŸŸ ", 
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢"
                }
                emoji = severity_emoji.get(issue.severity, "âšª")
                print(f"    {emoji} {issue.description}")
                print(f"       å½±å“: {issue.impact}")
                print(f"       æ ¹å› : {issue.root_cause}")
                print(f"       è§£å†³æ–¹æ¡ˆ: {issue.solution}")
                print()
            
            # ç”Ÿæˆä¼˜åŒ–è·¯çº¿å›¾
            roadmap = analyzer.generate_optimization_roadmap(issues)
            
            print(f"  ğŸ—ºï¸ ä¼˜åŒ–è·¯çº¿å›¾:")
            print(f"    æ€»é—®é¢˜æ•°: {roadmap['total_issues']}")
            print(f"    å…³é”®é—®é¢˜: {roadmap['critical_issues']}ä¸ª")
            print(f"    é«˜ä¼˜å…ˆçº§: {roadmap['high_priority_issues']}ä¸ª")
            
            print(f"\n  ğŸ“… åˆ†é˜¶æ®µä¼˜åŒ–è®¡åˆ’:")
            for phase_name, phase_info in roadmap['phases'].items():
                if phase_info['issues']:
                    print(f"    {phase_name}: {phase_info['timeline']} - {phase_info['focus']}")
                    for issue in phase_info['issues']:
                        print(f"      â€¢ {issue.description}")
            
            print(f"\n  ğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
            improvement = roadmap['estimated_improvement']
            print(f"    ç‚¹å‡»ç‡æå‡: +{improvement['ctr_improvement']:.3f}")
            print(f"    è½¬åŒ–ç‡æå‡: +{improvement['conversion_improvement']:.3f}")
            print(f"    å“åº”æ—¶é—´ä¼˜åŒ–: -{improvement['latency_improvement']:.3f}s")
            print(f"    é”™è¯¯ç‡é™ä½: -{improvement['error_rate_improvement']:.3f}")
            print(f"    æ€»ä½“æ”¹è¿›: +{improvement['overall_improvement']:.3f}")
            
            print(f"\n  ğŸ’° èµ„æºéœ€æ±‚:")
            resources = roadmap['resource_requirements']
            print(f"    å¼€å‘å·¥ä½œé‡: {resources['developer_weeks']:.1f} å‘¨")
            print(f"    ä¼˜å…ˆçº§: {resources['priority']}")
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            analysis_file = f"logs/performance_analysis_{timestamp}.json"
            analyzer.save_analysis_report(issues, roadmap, analysis_file)
            print(f"\n  ğŸ’¾ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {analysis_file}")
            
        else:
            print("  ğŸ‰ æ­å–œ! æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½!")
            
    except Exception as e:
        print(f"  âš ï¸ æ·±åº¦æ€§èƒ½åˆ†æå¤±è´¥: {e}")

def save_optimization_plan(plans: list, evaluation_report: dict):
    """ä¿å­˜ä¼˜åŒ–æ–¹æ¡ˆåˆ°æ–‡ä»¶"""
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¼˜åŒ–æ–¹æ¡ˆ
        optimization_file = f"logs/optimization_plan_{timestamp}.json"
        optimization_data = {
            "timestamp": datetime.now().isoformat(),
            "evaluation_summary": evaluation_report['summary'],
            "optimization_plans": plans,
            "priority": "high" if evaluation_report['summary']['overall_score'] < 2.5 else "medium"
        }
        
        with open(optimization_file, 'w', encoding='utf-8') as f:
            json.dump(optimization_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ ä¼˜åŒ–æ–¹æ¡ˆå·²ä¿å­˜åˆ°: {optimization_file}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ä¼˜åŒ–æ–¹æ¡ˆå¤±è´¥: {e}")

def run_benchmark_comparison():
    """è¿è¡Œbenchmarkå¯¹æ¯”åˆ†æ"""
    print("\nğŸ“Š Benchmarkå¯¹æ¯”åˆ†æ")
    print("-" * 40)
    
    try:
        # åˆ›å»ºè¯„ä¼°æŒ‡æ ‡å®ä¾‹
        evaluator = EvaluationMetrics()
        
        # æ¨¡æ‹Ÿä¸åŒåœºæ™¯çš„æ€§èƒ½æ•°æ®
        scenarios = {
            "å½“å‰ç³»ç»Ÿ": {
                "click_through_rate": 0.045,
                "conversion_rate": 0.08,
                "revenue_per_user": 1.50,
                "response_time": 0.25,
                "user_satisfaction": 4.2
            },
            "è¡Œä¸šå¹³å‡": {
                "click_through_rate": 0.05,
                "conversion_rate": 0.10,
                "revenue_per_user": 1.80,
                "response_time": 0.30,
                "user_satisfaction": 4.0
            },
            "è¡Œä¸šé¢†å…ˆ": {
                "click_through_rate": 0.08,
                "conversion_rate": 0.15,
                "revenue_per_user": 2.50,
                "response_time": 0.10,
                "user_satisfaction": 4.5
            }
        }
        
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        for scenario_name, metrics in scenarios.items():
            print(f"\n{scenario_name}:")
            for metric_name, value in metrics.items():
                benchmark_result = evaluator.evaluate_against_benchmark(metric_name, value)
                status_emoji = {
                    "excellent": "ğŸŸ¢",
                    "good": "ğŸŸ¡",
                    "acceptable": "ğŸŸ ",
                    "needs_improvement": "ğŸ”´"
                }
                emoji = status_emoji.get(benchmark_result.status, "âšª")
                print(f"  {emoji} {metric_name}: {value:.3f} ({benchmark_result.status})")
        
        # è®¡ç®—æ”¹è¿›ç©ºé—´
        current = scenarios["å½“å‰ç³»ç»Ÿ"]
        leading = scenarios["è¡Œä¸šé¢†å…ˆ"]
        
        print(f"\nğŸ¯ æ”¹è¿›ç©ºé—´åˆ†æ:")
        for metric_name in current.keys():
            if current[metric_name] > 0:
                improvement = ((leading[metric_name] - current[metric_name]) / current[metric_name]) * 100
                print(f"  {metric_name}: æå‡ç©ºé—´ {improvement:+.1f}%")
        
    except Exception as e:
        print(f"âš ï¸ Benchmarkå¯¹æ¯”åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œå…¨é¢è¯„ä¼°
    run_comprehensive_evaluation()
    
    # è¿è¡Œbenchmarkå¯¹æ¯”
    run_benchmark_comparison()
