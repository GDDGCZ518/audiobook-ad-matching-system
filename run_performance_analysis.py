#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºåˆ†æç³»ç»Ÿæ€§èƒ½é—®é¢˜å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.monitoring_analysis.performance_analyzer import PerformanceAnalyzer
from src.monitoring_analysis.evaluation_metrics import EvaluationMetrics

def run_performance_analysis():
    """è¿è¡Œæ€§èƒ½åˆ†æ"""
    print("ğŸ” æœ‰å£°ä¹¦å¹¿å‘ŠåŒ¹é…ç³»ç»Ÿ - æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
        print("ğŸ“Š åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨...")
        analyzer = PerformanceAnalyzer()
        
        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶ä»ç³»ç»Ÿè·å–ï¼‰
        print("ğŸ“ˆ åŠ è½½æ€§èƒ½æ•°æ®...")
        performance_data = {
            "click_through_rate": 0.024,      # å½“å‰ç‚¹å‡»ç‡
            "conversion_rate": 0.008,         # å½“å‰è½¬åŒ–ç‡
            "response_time": 2.163,           # å½“å‰å“åº”æ—¶é—´
            "error_rate": 0.015               # å½“å‰é”™è¯¯ç‡
        }
        
        print(f"å½“å‰æ€§èƒ½æŒ‡æ ‡:")
        for metric, value in performance_data.items():
            print(f"  {metric}: {value}")
        
        # åˆ†ææ€§èƒ½é—®é¢˜
        print(f"\nğŸ” å¼€å§‹æ€§èƒ½é—®é¢˜åˆ†æ...")
        issues = analyzer.analyze_performance_issues(performance_data)
        
        if issues:
            print(f"ğŸ“Š å‘ç° {len(issues)} ä¸ªæ€§èƒ½é—®é¢˜:")
            print("-" * 40)
            
            for i, issue in enumerate(issues):
                severity_emoji = {
                    "critical": "ğŸ”´",
                    "high": "ğŸŸ ", 
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢"
                }
                emoji = severity_emoji.get(issue.severity, "âšª")
                
                print(f"{emoji} é—®é¢˜ {i+1}: {issue.description}")
                print(f"   ä¸¥é‡ç¨‹åº¦: {issue.severity}")
                print(f"   ä¼˜å…ˆçº§: {issue.priority}")
                print(f"   å½±å“: {issue.impact}")
                print(f"   æ ¹å› : {issue.root_cause}")
                print(f"   è§£å†³æ–¹æ¡ˆ: {issue.solution}")
                print()
            
            # ç”Ÿæˆä¼˜åŒ–è·¯çº¿å›¾
            print("ğŸ—ºï¸ ç”Ÿæˆä¼˜åŒ–è·¯çº¿å›¾...")
            roadmap = analyzer.generate_optimization_roadmap(issues)
            
            print(f"ğŸ“Š è·¯çº¿å›¾æ¦‚è§ˆ:")
            print(f"  æ€»é—®é¢˜æ•°: {roadmap['total_issues']}")
            print(f"  å…³é”®é—®é¢˜: {roadmap['critical_issues']}ä¸ª")
            print(f"  é«˜ä¼˜å…ˆçº§é—®é¢˜: {roadmap['high_priority_issues']}ä¸ª")
            
            # æ˜¾ç¤ºåˆ†é˜¶æ®µè®¡åˆ’
            print(f"\nğŸ“… åˆ†é˜¶æ®µä¼˜åŒ–è®¡åˆ’:")
            for phase_name, phase_info in roadmap['phases'].items():
                if phase_info['issues']:
                    print(f"  {phase_name}: {phase_info['timeline']} - {phase_info['focus']}")
                    for issue in phase_info['issues']:
                        print(f"    â€¢ {issue.description}")
            
            # æ˜¾ç¤ºé¢„æœŸæ”¹è¿›æ•ˆæœ
            print(f"\nğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
            improvement = roadmap['estimated_improvement']
            print(f"  ç‚¹å‡»ç‡æå‡: +{improvement['ctr_improvement']:.3f}")
            print(f"  è½¬åŒ–ç‡æå‡: +{improvement['conversion_improvement']:.3f}")
            print(f"  å“åº”æ—¶é—´ä¼˜åŒ–: -{improvement['latency_improvement']:.3f}s")
            print(f"  é”™è¯¯ç‡é™ä½: -{improvement['error_rate_improvement']:.3f}")
            print(f"  æ€»ä½“æ”¹è¿›: +{improvement['overall_improvement']:.3f}")
            
            # æ˜¾ç¤ºèµ„æºéœ€æ±‚
            print(f"\nğŸ’° èµ„æºéœ€æ±‚:")
            resources = roadmap['resource_requirements']
            print(f"  å¼€å‘å·¥ä½œé‡: {resources['developer_weeks']:.1f} å‘¨")
            print(f"  ä¼˜å…ˆçº§: {resources['priority']}")
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            analysis_file = f"logs/performance_analysis_{timestamp}.json"
            analyzer.save_analysis_report(issues, roadmap, analysis_file)
            print(f"\nğŸ’¾ æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {analysis_file}")
            
            # ç”Ÿæˆå…·ä½“çš„ä¼˜åŒ–å»ºè®®
            print(f"\nğŸ’¡ å…·ä½“ä¼˜åŒ–å»ºè®®:")
            generate_specific_recommendations(issues)
            
        else:
            print("ğŸ‰ æ­å–œ! æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½!")
        
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def generate_specific_recommendations(issues):
    """ç”Ÿæˆå…·ä½“çš„ä¼˜åŒ–å»ºè®®"""
    print("  ğŸ“‹ è¯¦ç»†ä¼˜åŒ–å»ºè®®:")
    
    for i, issue in enumerate(issues):
        print(f"\n  {i+1}. {issue.description}")
        
        if issue.issue_type.value == "low_ctr":
            print("     å…·ä½“æªæ–½:")
            print("       â€¢ ä¼˜åŒ–æ¨èç®—æ³•å‚æ•°ï¼Œæé«˜åŒ¹é…ç²¾åº¦")
            print("       â€¢ æ”¹è¿›ç”¨æˆ·ç”»åƒç‰¹å¾å·¥ç¨‹")
            print("       â€¢ A/Bæµ‹è¯•ä¸åŒå¹¿å‘Šåˆ›æ„")
            print("       â€¢ ä¼˜åŒ–å¹¿å‘Šå±•ç¤ºä½ç½®å’Œæ—¶æœº")
            
        elif issue.issue_type.value == "low_conversion":
            print("     å…·ä½“æªæ–½:")
            print("       â€¢ é‡å†™å¹¿å‘Šæ–‡æ¡ˆï¼Œå¢å¼ºå¸å¼•åŠ›")
            print("       â€¢ ä¼˜åŒ–è½åœ°é¡µè®¾è®¡å’Œç”¨æˆ·ä½“éªŒ")
            print("       â€¢ ç®€åŒ–ç”¨æˆ·è½¬åŒ–æµç¨‹")
            print("       â€¢ å¢åŠ ç”¨æˆ·å¼•å¯¼å’Œæ¿€åŠ±")
            
        elif issue.issue_type.value == "high_latency":
            print("     å…·ä½“æªæ–½:")
            print("       â€¢ ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œå‡å°‘è®¡ç®—é‡")
            print("       â€¢ å®ç°æ™ºèƒ½ç¼“å­˜ç­–ç•¥")
            print("       â€¢ å¢åŠ å¹¶è¡Œå¤„ç†å’Œå¼‚æ­¥å¤„ç†")
            print("       â€¢ ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•")
            
        elif issue.issue_type.value == "high_error_rate":
            print("     å…·ä½“æªæ–½:")
            print("       â€¢ å®Œå–„å¼‚å¸¸å¤„ç†æœºåˆ¶")
            print("       â€¢ å¢åŠ ç³»ç»Ÿç›‘æ§å’Œå‘Šè­¦")
            print("       â€¢ å®ç°è‡ªåŠ¨é”™è¯¯æ¢å¤")
            print("       â€¢ æ”¹è¿›ä»£ç è´¨é‡å’Œæµ‹è¯•è¦†ç›–")

def run_benchmark_analysis():
    """è¿è¡Œbenchmarkåˆ†æ"""
    print("\nğŸ“Š Benchmarkåˆ†æ")
    print("-" * 40)
    
    try:
        # åˆ›å»ºè¯„ä¼°æŒ‡æ ‡å®ä¾‹
        evaluator = EvaluationMetrics()
        
        # æ¨¡æ‹Ÿä¸åŒåœºæ™¯çš„æ€§èƒ½æ•°æ®
        scenarios = {
            "å½“å‰ç³»ç»Ÿ": {
                "click_through_rate": 0.024,
                "conversion_rate": 0.008,
                "response_time": 2.163,
                "error_rate": 0.015
            },
            "è¡Œä¸šå¹³å‡": {
                "click_through_rate": 0.050,
                "conversion_rate": 0.100,
                "response_time": 0.300,
                "error_rate": 0.010
            },
            "è¡Œä¸šé¢†å…ˆ": {
                "click_through_rate": 0.080,
                "conversion_rate": 0.150,
                "response_time": 0.100,
                "error_rate": 0.005
            }
        }
        
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
        for scenario_name, metrics in scenarios.items():
            print(f"\n{scenario_name}:")
            for metric_name, value in metrics.items():
                benchmark_result = evaluator.evaluate_against_benchmark(metric_name, value)
                status_emoji = {
                    "excellent": "ğŸŸ¢",
                    "good": "ğŸŸ¡",
                    "acceptable": "ğŸŸ ",
                    "needs_improvement": "ğŸ”´"
                }
                emoji = status_emoji.get(benchmark_result.status, "âšª")
                print(f"  {emoji} {metric_name}: {value:.3f} ({benchmark_result.status})")
        
        # è®¡ç®—æ”¹è¿›ç©ºé—´
        current = scenarios["å½“å‰ç³»ç»Ÿ"]
        leading = scenarios["è¡Œä¸šé¢†å…ˆ"]
        
        print(f"\nğŸ¯ æ”¹è¿›ç©ºé—´åˆ†æ:")
        for metric_name in current.keys():
            if current[metric_name] > 0:
                if metric_name == "response_time":
                    # å“åº”æ—¶é—´æ˜¯è¶Šå°è¶Šå¥½
                    improvement = ((current[metric_name] - leading[metric_name]) / current[metric_name]) * 100
                    print(f"  {metric_name}: ä¼˜åŒ–ç©ºé—´ {improvement:+.1f}%")
                else:
                    # å…¶ä»–æŒ‡æ ‡æ˜¯è¶Šå¤§è¶Šå¥½
                    improvement = ((leading[metric_name] - current[metric_name]) / current[metric_name]) * 100
                    print(f"  {metric_name}: æå‡ç©ºé—´ {improvement:+.1f}%")
        
    except Exception as e:
        print(f"âš ï¸ Benchmarkåˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½åˆ†æ
    run_performance_analysis()
    
    # è¿è¡Œbenchmarkåˆ†æ
    run_benchmark_analysis()
